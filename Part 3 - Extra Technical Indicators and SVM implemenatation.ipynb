{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GOAL OF PROJECT\n",
    "\n",
    "To implement the following paper \"Predicting the direction of stock market prices using random forest\" - Luckyson Khaidem, Snehanshu Saha, Sudeepa Roy Dey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTING THE LIBRARIES REQUIRED FOR THE TASK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model of choice for training and prediction purposes is the Random Forest Classifier\n",
    "\n",
    "### Importing required libraries for training and testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, precision_score, confusion_matrix, recall_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In order to use the technical financial indicators required in the paper, we utilise a pre-defined library as follows:\n",
    "\n",
    "## https://github.com/Crypto-toolbox/pandas-technical-indicators/blob/master/technical_indicators.py\n",
    "\n",
    "#### Importing the above library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_techinal_indicators as ta \n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We are using data from a variety of pharma companies to form trend for the market "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we are trying to predict the movements of CIPLA given the data for other related stocks\n",
    "\n",
    "raw_data = pd.read_csv('CIPLA.csv')\n",
    "\n",
    "del(raw_data['Series'])\n",
    "del(raw_data['Date'])\n",
    "del(raw_data['Symbol'])\n",
    "del(raw_data['N'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data2 = pd.read_csv('CADILAHC.csv')\n",
    "\n",
    "del(raw_data2['Series'])\n",
    "del(raw_data2['Date'])\n",
    "del(raw_data2['Symbol'])\n",
    "del(raw_data2['N'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data3 = pd.read_csv('DRREDDY.csv')\n",
    "\n",
    "del(raw_data3['Series'])\n",
    "del(raw_data3['Date'])\n",
    "del(raw_data3['Symbol'])\n",
    "del(raw_data3['N'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data4 = pd.read_csv('LUPIN.csv')\n",
    "\n",
    "del(raw_data4['Series'])\n",
    "del(raw_data4['Date'])\n",
    "del(raw_data4['Symbol'])\n",
    "del(raw_data4['N'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data5 = pd.read_csv('SUNPHARMA.csv')\n",
    "\n",
    "del(raw_data5['Series'])\n",
    "del(raw_data5['Date'])\n",
    "del(raw_data5['Symbol'])\n",
    "del(raw_data5['N'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exponential Smoothing \n",
    "\n",
    "As indicated by the authors of the paper, this is done for the purpose of putting more importance on recent data and exponentially decreasing weightage to past data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for exponentially smoothing\n",
    "\n",
    "def exp_smoothing(df, alpha):\n",
    "    es_data = df.ewm(alpha=alpha).mean()    \n",
    "    return es_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prev Close</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Last</th>\n",
       "      <th>Close</th>\n",
       "      <th>Average</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Turnover</th>\n",
       "      <th>No. of Trades</th>\n",
       "      <th>Deliverable Qty</th>\n",
       "      <th>% Dly Qt to Traded Qty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>626.400000</td>\n",
       "      <td>626.500000</td>\n",
       "      <td>634.800000</td>\n",
       "      <td>626.500000</td>\n",
       "      <td>629.950000</td>\n",
       "      <td>628.400000</td>\n",
       "      <td>630.720000</td>\n",
       "      <td>5.967870e+05</td>\n",
       "      <td>3.764074e+08</td>\n",
       "      <td>9333.000000</td>\n",
       "      <td>201185.000000</td>\n",
       "      <td>33.710000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>628.218182</td>\n",
       "      <td>632.409091</td>\n",
       "      <td>637.300000</td>\n",
       "      <td>629.090909</td>\n",
       "      <td>630.904545</td>\n",
       "      <td>629.990909</td>\n",
       "      <td>632.756364</td>\n",
       "      <td>6.806161e+05</td>\n",
       "      <td>4.306800e+08</td>\n",
       "      <td>10921.181818</td>\n",
       "      <td>320182.272727</td>\n",
       "      <td>46.882727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>629.958559</td>\n",
       "      <td>626.635135</td>\n",
       "      <td>637.930631</td>\n",
       "      <td>626.306306</td>\n",
       "      <td>632.251802</td>\n",
       "      <td>632.701802</td>\n",
       "      <td>633.417297</td>\n",
       "      <td>1.223210e+06</td>\n",
       "      <td>7.748409e+08</td>\n",
       "      <td>21567.864865</td>\n",
       "      <td>652569.414414</td>\n",
       "      <td>53.042432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>632.696130</td>\n",
       "      <td>630.563906</td>\n",
       "      <td>632.592529</td>\n",
       "      <td>612.079208</td>\n",
       "      <td>615.778533</td>\n",
       "      <td>616.183528</td>\n",
       "      <td>621.124500</td>\n",
       "      <td>1.576297e+06</td>\n",
       "      <td>9.785972e+08</td>\n",
       "      <td>53017.131413</td>\n",
       "      <td>956747.259226</td>\n",
       "      <td>60.483987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>616.184448</td>\n",
       "      <td>615.971245</td>\n",
       "      <td>619.279120</td>\n",
       "      <td>605.257853</td>\n",
       "      <td>616.067856</td>\n",
       "      <td>612.373315</td>\n",
       "      <td>612.462363</td>\n",
       "      <td>8.901653e+05</td>\n",
       "      <td>5.458053e+08</td>\n",
       "      <td>36130.144271</td>\n",
       "      <td>429642.354874</td>\n",
       "      <td>47.079265</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Prev Close        Open        High         Low        Last       Close  \\\n",
       "0  626.400000  626.500000  634.800000  626.500000  629.950000  628.400000   \n",
       "1  628.218182  632.409091  637.300000  629.090909  630.904545  629.990909   \n",
       "2  629.958559  626.635135  637.930631  626.306306  632.251802  632.701802   \n",
       "3  632.696130  630.563906  632.592529  612.079208  615.778533  616.183528   \n",
       "4  616.184448  615.971245  619.279120  605.257853  616.067856  612.373315   \n",
       "\n",
       "      Average        Volume      Turnover  No. of Trades  Deliverable Qty  \\\n",
       "0  630.720000  5.967870e+05  3.764074e+08    9333.000000    201185.000000   \n",
       "1  632.756364  6.806161e+05  4.306800e+08   10921.181818    320182.272727   \n",
       "2  633.417297  1.223210e+06  7.748409e+08   21567.864865    652569.414414   \n",
       "3  621.124500  1.576297e+06  9.785972e+08   53017.131413    956747.259226   \n",
       "4  612.462363  8.901653e+05  5.458053e+08   36130.144271    429642.354874   \n",
       "\n",
       "   % Dly Qt to Traded Qty  \n",
       "0               33.710000  \n",
       "1               46.882727  \n",
       "2               53.042432  \n",
       "3               60.483987  \n",
       "4               47.079265  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For current testing purposes, value of alpha used is 0.9\n",
    "\n",
    "sdata = exp_smoothing(raw_data, 0.9)\n",
    "sdata2 = exp_smoothing(raw_data2, 0.9)\n",
    "sdata3 = exp_smoothing(raw_data3, 0.9)\n",
    "sdata4 = exp_smoothing(raw_data4, 0.9)\n",
    "sdata5 = exp_smoothing(raw_data5, 0.9)\n",
    "# Let us visualise the data\n",
    "\n",
    "sdata.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technical indicators used for Feature Extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(data):\n",
    "    for x in [5, 14, 26, 44, 66]:\n",
    "        data = ta.relative_strength_index(data, n=x)\n",
    "        data = ta.stochastic_oscillator_d(data, n=x)\n",
    "        data = ta.accumulation_distribution(data, n=x)\n",
    "        data = ta.average_true_range(data, n=x)\n",
    "        data = ta.momentum(data, n=x)\n",
    "        data = ta.money_flow_index(data, n=x)\n",
    "        data = ta.rate_of_change(data, n=x)\n",
    "        data = ta.on_balance_volume(data, n=x)\n",
    "        data = ta.commodity_channel_index(data, n=x)\n",
    "        data = ta.ease_of_movement(data, n=x)\n",
    "        data = ta.trix(data, n=x)\n",
    "        data = ta.vortex_indicator(data, n=x)\n",
    "        data = ta.moving_average(data, n=x)\n",
    "        data = ta.standard_deviation(data, n=x) \n",
    "        data = ta.keltner_channel(data, n=x)\n",
    "        data = ta.coppock_curve(data, n=x)\n",
    "        data = ta.force_index(data, n=x)\n",
    "        data = ta.bollinger_bands(data, n=x)\n",
    "        data = ta.exponential_moving_average(data, n=x)\n",
    "    \n",
    "    data = ta.ppsr(data)\n",
    "    data = ta.stochastic_oscillator_k(data)\n",
    "    data = ta.mass_index(data)\n",
    "    data = ta.ultimate_oscillator(data)\n",
    "    data['ema50'] = data['Close'] / data['Close'].ewm(50).mean()\n",
    "    data['ema21'] = data['Close'] / data['Close'].ewm(21).mean()\n",
    "    data['ema14'] = data['Close'] / data['Close'].ewm(14).mean()\n",
    "    data['ema5'] = data['Close'] / data['Close'].ewm(5).mean()\n",
    "    data = ta.chaikin_oscillator(data)    \n",
    "         \n",
    "    data = ta.macd(data, n_fast=12, n_slow=26)\n",
    "    \n",
    "    del(data['Open'])\n",
    "    del(data['Prev Close'])\n",
    "    del(data['High'])\n",
    "    del(data['Low'])\n",
    "    del(data['Volume'])\n",
    "    del(data['Last'])\n",
    "    del(data['Average'])\n",
    "    del(data['Turnover'])\n",
    "    del(data['No. of Trades'])\n",
    "    del(data['Deliverable Qty'])\n",
    "    del(data['% Dly Qt to Traded Qty'])\n",
    "    \n",
    "    return data\n",
    "   \n",
    "def compute_prediction_int(df, n):\n",
    "    pred = (df.shift(-n)['Close'] >= df['Close'])\n",
    "    pred = pred.iloc[:-n]\n",
    "    return pred.astype(int)\n",
    "\n",
    "def prepare_data(df, horizon):\n",
    "    data = feature_extraction(df).dropna().iloc[:-horizon]\n",
    "    data['pred'] = compute_prediction_int(data, n=horizon)\n",
    "    del(data['Close'])\n",
    "    return data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation of training data and labels.\n",
    "\n",
    "### Assume a prediction horizon of 1 days "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data done\n",
      "data2 done\n",
      "data3 done\n",
      "data4 done\n",
      "data5 done\n",
      "suffixes added\n"
     ]
    }
   ],
   "source": [
    "data = prepare_data(sdata, 1)\n",
    "print('data done')\n",
    "data2 = prepare_data(sdata2, 1)\n",
    "print('data2 done')\n",
    "data3 = prepare_data(sdata3, 1)\n",
    "print('data3 done')\n",
    "data4 = prepare_data(sdata4, 1)\n",
    "print('data4 done')\n",
    "data5 = prepare_data(sdata5, 1)\n",
    "print('data5 done')\n",
    "\n",
    "\n",
    "## Identifying and extracting the label\n",
    "data2 = data2.add_suffix('_2')\n",
    "data3 = data3.add_suffix('_3')\n",
    "data4 = data4.add_suffix('_4')\n",
    "data5 = data5.add_suffix('_5')\n",
    "\n",
    "print('suffixes added')\n",
    "\n",
    "\n",
    "data = pd.concat([data, data2], axis=1)\n",
    "data = pd.concat([data, data3], axis=1)\n",
    "data = pd.concat([data, data4], axis=1)\n",
    "data = pd.concat([data, data5], axis=1)\n",
    "\n",
    "## Extracting the input features and creating the input feature matrix\n",
    "\n",
    "data = data.dropna()\n",
    "del(data['pred_2'])\n",
    "del(data['pred_3'])\n",
    "del(data['pred_4'])\n",
    "del(data['pred_5'])\n",
    "\n",
    "input_feature = [x for x in data.columns if x not in ['gain', 'pred']]\n",
    "X = data[input_feature]\n",
    "y = data['pred']\n",
    "\n",
    "#print(\"This is X: \")\n",
    "#print(X)\n",
    "#print(\"This is y: \")\n",
    "#print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling the data into range (-1,1) for pre-processing  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_new = scaler.fit_transform(X)\n",
    "\n",
    "#print(\"This is X_new: \")\n",
    "#print(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HERE, instead of splitting into training and testing radnomly (as is done usually), we are using the TimeSeriesSplit method to split the data into training and testing data\n",
    "\n",
    "### This is because, since we are using time series data, random splitting may lead to data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len X_train 946\n",
      "len y_train 946\n",
      "len X_test 189\n",
      "len y_test 189\n"
     ]
    }
   ],
   "source": [
    "tscv = TimeSeriesSplit()\n",
    "\n",
    "for train_index, test_index in tscv.split(X_new):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X_new[train_index, :], X_new[test_index,:]\n",
    "    \n",
    "for train_index, test_index in tscv.split(y):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "## Printing sizes to verify correctness\n",
    "print('len X_train', len(X_train))\n",
    "print('len y_train', len(y_train))\n",
    "print('len X_test', len(X_test))\n",
    "print('len y_test', len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL - Random Forests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_jobs=-1, n_estimators=111, random_state=42)\n",
    "\n",
    "## @EVERYONE - Try playing with the parameters above - increasing number of estimators may help improving accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model fitted\n",
      "prediction created\n",
      "189\n",
      "189\n",
      "Accuracy: 0.54\n",
      "Confusion Matrix\n",
      "[[63 40]\n",
      " [47 39]]\n",
      "Precision: 0.49, Recall: 0.45, f1: 0.47\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train.values.ravel());\n",
    "print(\"model fitted\")\n",
    "prediction = model.predict(X_test)\n",
    "print(\"prediction created\")\n",
    "print(np.size(prediction))\n",
    "print(np.size(y_test))\n",
    "\n",
    "accuracy = accuracy_score(y_pred=prediction, y_true=y_test)\n",
    "\n",
    "print('Accuracy: {0:1.2f}'.format(accuracy))\n",
    "\n",
    "\n",
    "confusion = confusion_matrix(y_pred=prediction, y_true=y_test)\n",
    "print('Confusion Matrix')\n",
    "print(confusion)\n",
    "\n",
    "\n",
    "precision = precision_score(y_pred=prediction, y_true=y_test)\n",
    "recall = recall_score(y_pred=prediction, y_true=y_test)\n",
    "f1 = f1_score(y_pred=prediction, y_true=y_test)\n",
    "print('Precision: {0:1.2f}, Recall: {1:1.2f}, f1: {2:1.2f}'.format(precision, recall, f1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRYING SVM IMPLEMENTATION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model fitted\n",
      "prediction created\n",
      "189\n",
      "189\n",
      "Accuracy: 0.58\n",
      "Confusion Matrix\n",
      "[[96  7]\n",
      " [72 14]]\n",
      "Precision: 0.67, Recall: 0.16, f1: 0.26\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "# model = svm.SVC(gamma = 1 / (X_train.shape[-1] * X_train.var()))\n",
    "\n",
    "model = svm.SVC()\n",
    "\n",
    "model.fit(X_train, y_train.values.ravel());\n",
    "print(\"model fitted\")\n",
    "prediction = model.predict(X_test)\n",
    "print(\"prediction created\")\n",
    "print(np.size(prediction))\n",
    "print(np.size(y_test))\n",
    "\n",
    "accuracy = accuracy_score(y_pred=prediction, y_true=y_test)\n",
    "\n",
    "print('Accuracy: {0:1.2f}'.format(accuracy))\n",
    "\n",
    "\n",
    "confusion = confusion_matrix(y_pred=prediction, y_true=y_test)\n",
    "print('Confusion Matrix')\n",
    "print(confusion)\n",
    "\n",
    "\n",
    "precision = precision_score(y_pred=prediction, y_true=y_test)\n",
    "recall = recall_score(y_pred=prediction, y_true=y_test)\n",
    "f1 = f1_score(y_pred=prediction, y_true=y_test)\n",
    "print('Precision: {0:1.2f}, Recall: {1:1.2f}, f1: {2:1.2f}'.format(precision, recall, f1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
